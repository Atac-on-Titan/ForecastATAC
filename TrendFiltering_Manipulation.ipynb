{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Package import\n",
    "\n",
    "First of all, we import all the necessary libraries, apart from the fundamental ones, there are some additional ones:\n",
    "\n",
    "- `cvxpy` for (convex) optimization, which is needed for trend filtering, which is a [Generalized Lasso problem](https://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf).\n",
    "- `folium` is a package useful to build the map of lines and stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import folium\n",
    "from typing import Optional"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:12.129206Z",
     "start_time": "2023-09-06T23:11:12.010527Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The static data\n",
    "- `stop_times` is a dataframe with stops (with related `stop_id`) for every and each `trip_id` and related `stop_sequence` number. It also has the cumulative distance traveled by the bus/subway/tram up until the specific stop (along the _shape_ describing the trip). \n",
    "- `trip_info` is a dataframe keeping the association between `route_id` and `trip_id`.\n",
    "- `stop_list` is a dataframe listing the stops. It is useful since it has latitude and longitude of every stop.\n",
    "- `routes` is a dataframe linking each route to its agency (ATAC, TPL or even Trenitalia) and to its type (bus, subway or tram).\n",
    "\n",
    "The `low_memory` option here is important, you don't want to have mixed type columns when performing joins ;)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "stop_times = pd.read_csv('data/static/stop_times.txt', low_memory=False)\n",
    "trip_info = pd.read_csv('data/static/trips.txt', low_memory=False)\n",
    "stop_list = pd.read_csv('data/static/stops.txt', low_memory=False)\n",
    "routes = pd.read_csv('data/static/routes.txt', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:15.431483Z",
     "start_time": "2023-09-06T23:11:12.016293Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graphs and maps\n",
    "\n",
    "## The first sequence of (inner) joins\n",
    "\n",
    "First of all, we perform the first inner join (of many) between the `trip_info` table and the `stop_times` one on `trip_id`; we want to put together information related to trips and stop sequences. In other words, this way we are recovering the sequence of stops for each trip, even distinguishing between directions with the `direction_id`, which is just a binary field. The `stop_sequence` integer is the variable allowing the sequence recovering.\n",
    "\n",
    "### What's a trip? \n",
    "\n",
    "Everything else is more or less clear, but we want to stress the difference between a _route_ and a _trip_ according to the [GTFS standard](https://developers.google.com/transit/gtfs/reference?hl=en). Specifically: \n",
    "> A trip is a sequence of two or more stops that occur during a specific time period.\n",
    "\n",
    "While: \n",
    "> A route is a group of trips that are displayed to riders as a single service.\n",
    "\n",
    "Therefore $\\text{trip} \\in \\text{route}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "complete = trip_info.merge(stop_times, how = 'inner', on = 'trip_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:16.308862Z",
     "start_time": "2023-09-06T23:11:15.433231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['route_id', 'service_id', 'trip_id', 'trip_headsign', 'trip_short_name',\n       'direction_id', 'block_id', 'shape_id', 'wheelchair_accessible',\n       'exceptional', 'arrival_time', 'departure_time', 'stop_id',\n       'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type',\n       'shape_dist_traveled', 'timepoint'],\n      dtype='object')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:16.316407Z",
     "start_time": "2023-09-06T23:11:16.309639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We drop everything that it is not needed for this stage. We keep `shape_id` since we want to use shapes later on. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "complete = complete[['route_id', 'trip_id', 'stop_id', 'stop_sequence', 'direction_id', 'shape_id']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:16.524720Z",
     "start_time": "2023-09-06T23:11:16.315364Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next inner join is to add route specific information (not trip specific). A route is what we call a _bus line_. This way we can filter out everything which is not handled by ATAC and subway/tram lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "complete = complete.merge(routes, on = 'route_id', how = 'inner')\n",
    "complete = complete[(complete['agency_id'] == 'OP1')&(complete['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:17.775732Z",
     "start_time": "2023-09-06T23:11:16.644679Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the next inner join we are recovering stop related information, like name, latitude and longitude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "complete = complete.merge(stop_list, how = 'inner', on = 'stop_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:18.810865Z",
     "start_time": "2023-09-06T23:11:17.876920Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we do not need `trip id`, we remove the column and drop the duplicates w.r.t. route, stop sequence and direction. At this stage, we only want to build (and plot) an undirected graph of ATAC public transport relying on buses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "complete = (complete.drop('trip_id', axis = 1).drop_duplicates(['route_id', 'stop_sequence', 'direction_id']).reset_index())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.576659Z",
     "start_time": "2023-09-06T23:11:18.811992Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having initialized the graph, we add nodes by simply reading the `stop_list` dataframe line by line. Then, we add edges by sorting by `stop_sequence` and by grouping by `route_id` and `direction_id` (`groupby` in Pandas preserves row ordering). Once done that, we can just iterate group by group, adding edges in a sequential order if not already present in the graph. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "init_graph = nx.Graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.578442Z",
     "start_time": "2023-09-06T23:11:19.577230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "for _, row in stop_list.iterrows():\n",
    "    init_graph.add_node(row['stop_id'], name = row['stop_name'], latitude = row['stop_lat'], longitude = row['stop_lon'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.776573Z",
     "start_time": "2023-09-06T23:11:19.580230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "routes_grouped = complete.sort_values(by='stop_sequence').groupby(['route_id', 'direction_id'])\n",
    "for _, group in routes_grouped:\n",
    "    stops = group['stop_id'].tolist()\n",
    "    edges = [(stops[i], stops[i+1]) for i in range(len(stops)-1) if not init_graph.has_edge(stops[i], stops[i+1])]\n",
    "    init_graph.add_edges_from(edges)\n",
    "    \n",
    "init_graph.remove_nodes_from(list(nx.isolates(init_graph))) # if present, we remove isolated nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.814655Z",
     "start_time": "2023-09-06T23:11:19.776306Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the graph over the map (with Folium)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are plotting the edges (not the vertexes, to avoid overplotting) with Folium; the map is centered in the Campidoglio (XD), with a moderate zoom. To do this, we are adding the edges through `Polyline` objects added to the baseline _terrain layer_ map. The map is then saved locally and can be opened with a browser. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "\n",
    "# for _, row in stop_list.iterrows():\n",
    "#     if init_graph.has_node(row['stop_id']):\n",
    "#         folium.Marker(\n",
    "#             location=[row['stop_lat'], row['stop_lon']],\n",
    "#             popup=f\"{row['stop_name']}\",\n",
    "#             icon = folium.Icon(prefix='fa', icon = 'bus', icon_size=(5, 5))\n",
    "#         ).add_to(rome)\n",
    "\n",
    "coords = [(edge[0], edge[1]) for edge in init_graph.edges()]\n",
    "for _, group in routes_grouped:\n",
    "    coords = [(lat, long) for lat, long in zip(group['stop_lat'], group['stop_lon'])]\n",
    "    folium.PolyLine(locations=coords, color='blue', weight=1, opacity=0.5).add_to(rome)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.858823Z",
     "start_time": "2023-09-06T23:11:19.813133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "rome.save('maps/test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:19.988485Z",
     "start_time": "2023-09-06T23:11:19.856009Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An improvement: use _shape_ data\n",
    "\n",
    "Instead of relying on latitude and longitude in order to visualize things, we use what Google Maps actually uses, AKA [_shape_](https://developers.google.com/transit/gtfs/reference?hl=en#shapestxt) data. This is not difficult to do, since we have a `shape.txt` file storing that information.\n",
    "\n",
    "This what we could when developing a possible application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "shapes = pd.read_csv('data/static/shapes.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:20.104846Z",
     "start_time": "2023-09-06T23:11:19.988582Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We remove the shapes of what does not belong to our target data (ATAC + buses)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "shapes = shapes[shapes['shape_id'].isin(complete['shape_id'])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:20.126327Z",
     "start_time": "2023-09-06T23:11:20.114303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "grouped = shapes.sort_values(by = 'shape_pt_sequence').groupby('shape_id')\n",
    "for _, group in grouped:\n",
    "    coords = [(lat, lon) for lat, lon in zip(group['shape_pt_lat'], group['shape_pt_lon'])]\n",
    "    folium.PolyLine(locations=coords, color = 'blue', weight = 1, opacity = 0.5).add_to(rome)\n",
    "    \n",
    "rome.save('maps/improved_test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:21.724524Z",
     "start_time": "2023-09-06T23:11:20.127946Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling live data\n",
    "\n",
    "Now, we read the real time data, the scraped one, if scraping can be the right wording for it, since it was just GET calls to the Rome GTFS feed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "trip_live = pd.read_feather('data/trip-updates.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:25.426440Z",
     "start_time": "2023-09-06T23:11:21.725323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "trip_live = trip_live.drop_duplicates(subset=['trip_id', 'start_time', 'start_date', 'stop_sequence'], keep = 'last', ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:30.473510Z",
     "start_time": "2023-09-06T23:11:25.587233Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 0 problem: the first stop of every trip has 0 as its timestamp: we need to remove those rows since they are basically useless."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "trip_live = trip_live[trip_live['time'] != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:31.522268Z",
     "start_time": "2023-09-06T23:11:30.474184Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again , we consider only the bus lines belonging to ATAC, and only bus lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(routes, on = 'route_id', how = 'inner')\n",
    "trip_live = trip_live[(trip_live['agency_id'] == 'OP1')&(trip_live['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:37.791982Z",
     "start_time": "2023-09-06T23:11:31.968987Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add weather"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add the weather data, collected with the [Open Weather history API](https://openweathermap.org/history)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "weather_main\nClouds          461\nClear           414\nRain             98\nThunderstorm     24\nMist              6\nDrizzle           3\nFog               2\nName: count, dtype: int64"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_feather('data/weather_df.feather')\n",
    "weather['weather_main'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:37.800237Z",
     "start_time": "2023-09-06T23:11:37.792757Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weather data is hourly, so we use floor division to move from the UNIX timestamp (in seconds) to the integer hours. This is the case for both real time data and weather. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "trip_live['hourly'] = trip_live['time'] // 3600\n",
    "weather['hourly'] = weather['timestamp'] // 3600\n",
    "weather.drop('timestamp', axis = 1, inplace=True)\n",
    "trip_live = trip_live.merge(weather, how='inner', on='hourly')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:41.507539Z",
     "start_time": "2023-09-06T23:11:37.800904Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Put together sequential updates\n",
    "\n",
    "Now we put together information related to sequential updates, each (disjoint) pair of records/stops in a trip is put together.\n",
    "\n",
    "The trick is copying the dataset and then shifting the `stop_sequence` by -1. At this point, an inner join between the two dataset (the copied and the original one) is enough. Suffixes `_pre` and `_post` are enough to distinguish the two sets of fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "trip_live_shifted = trip_live.copy()\n",
    "trip_live_shifted['stop_sequence'] = trip_live_shifted['stop_sequence'] - 1\n",
    "trip_live = trip_live.merge(trip_live_shifted, how = 'inner', on = ['route_id', 'trip_id', 'start_time', 'start_date', 'stop_sequence'], suffixes=('_pre', '_post'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:03.726845Z",
     "start_time": "2023-09-06T23:11:41.507429Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add stop info and (actual) distances\n",
    "\n",
    "Now we add stop info, both for what concerns stop specific information and for what concerns the stop sequence. With `stop_times` we also get the [travelled distance in-between stops](https://developers.google.com/transit/gtfs/reference?hl=en#stop_timestxt). The problem is that the foreign key for `stop_times` is trip-related, and static data on trips is continuously updated (once a day); therefore, we cannot really do an inner join based on `trip_id` for what concerns dynamic/scraped data, in **any case**.\n",
    "\n",
    "The idea is the following:\n",
    "\n",
    "- You first perform an inner join between `stop_times` file and `trip_info`, based on `trip_id`s; we are more or less sure that this is consistent, since the text files come from the same GET request.\n",
    "- We again use the trick of subtracting 1 to the stop sequence in order to join two stops in a sequence in the same row. This is basically an inner join of the dataframe with itself, in a way.\n",
    "- We end up with a dataframe linking `route_id`s, `stop_id`s of the departing stops, `stop_id`s of the arrival stops and the distance travelled along the shape for each piece of route/edge. \n",
    "- At this point, finally, we can perform an inner join between the `trip_live` dataframe and the one we have been building here, using as key four attributes: [`route_id`, `stop_id_pre`, `stop_id_post`].\n",
    "\n",
    "After everything, we simply compute the actual distance between stops by taking the difference between the cumulative traveled distance fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "route_stop = trip_info.merge(stop_times, on='trip_id')\n",
    "route_stop_shifted = route_stop.copy()\n",
    "route_stop_shifted['stop_sequence'] -= 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:05.364324Z",
     "start_time": "2023-09-06T23:12:03.726392Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "route_stop = route_stop.merge(route_stop_shifted, on = ['trip_id', 'route_id', 'stop_sequence'], suffixes=('_pre', '_post'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:08.017028Z",
     "start_time": "2023-09-06T23:12:05.366131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "route_stop.rename(columns={'stop_sequence':'stop_sequence_pre'}, inplace=True)\n",
    "route_stop = route_stop[['route_id', 'stop_id_pre', 'stop_id_post', 'shape_dist_traveled_pre', 'shape_dist_traveled_post']]\n",
    "route_stop = route_stop.drop_duplicates(subset=['route_id', 'stop_id_pre', 'stop_id_post'], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:08.640250Z",
     "start_time": "2023-09-06T23:12:08.018970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(route_stop, on=['route_id', 'stop_id_pre', 'stop_id_post'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:20.421825Z",
     "start_time": "2023-09-06T23:12:08.985375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "trip_live['stop_distance'] = trip_live['shape_dist_traveled_post']-trip_live['shape_dist_traveled_post']\n",
    "del route_stop, route_stop_shifted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:20.585676Z",
     "start_time": "2023-09-06T23:12:20.419980Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add day of the week\n",
    "\n",
    "We add the day of the week (as integer) in order to be more specific when building the (filtered) graph. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "trip_live['time_pre_datetime'] = pd.to_datetime(trip_live['time_pre'], origin='unix', unit = 's')\n",
    "trip_live['day_of_week'] = trip_live.time_pre_datetime.dt.weekday"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:21.455337Z",
     "start_time": "2023-09-06T23:12:20.592221Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add time\n",
    "We take the difference between (prediction) timestamps to get the elapsed time between stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "trip_live['elapsed'] = trip_live['time_post']-trip_live['time_pre']\n",
    "trip_live = trip_live[trip_live['elapsed'] >= 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:25.248023Z",
     "start_time": "2023-09-06T23:12:21.456490Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before continuing, we remove from `trip_live` the columns which are not necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "trip_live = trip_live[['weather_main_post', 'day_of_week', 'time_pre_datetime', 'elapsed', 'stop_distance', 'stop_id_post']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:26.266613Z",
     "start_time": "2023-09-06T23:12:25.253533Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "trip_live.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:15:56.793293Z",
     "start_time": "2023-09-06T23:15:56.777779Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "trip_live.to_feather('trip_live.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T23:17:00.502304Z",
     "start_time": "2023-09-06T23:16:59.620508Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function to build the signal over the graph vertexes\n",
    "The following is a function to define the signal over the vertexes. The way we are defining the signal over the vertex set is the following: \n",
    "> The signal for each vertex is the average value (averaging along the observations gathered through some filtering options) of the elapsed time to get to a specific vertex from the previous divided by the (shape) distance between the previous vertex to the next.\n",
    "\n",
    "The resulting graph is thus related only to a specific filtering option (based on weather, day of the week and time of the day). We allow the choice of only one filtering option, in order to have enough observations to estimate meaningful average of the signal instances. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def vertex_signal(complete_df: pd.DataFrame, routes_graph: nx.Graph, *, wtr: Optional[str] = None, dow: Optional[int] = None, daytime:Optional[tuple[int, int]] = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Function assigning signal over the public transport graph vertexes by averaging across the inbound edges \n",
    "    elapsed time, according to a specific filtering option, passed as keyword argument.\n",
    "    :param complete_df: The dataframe containing the preprocessed data.\n",
    "    :param routes_graph: The graph, already built.\n",
    "    :param wtr: Main weather conditions.\n",
    "    :param dow: The day of the week, as integer.\n",
    "    :param daytime: The daytime, as an interval specified by a tuple of two integers.\n",
    "    :return: The graph with the signal defined over the vertex set.\n",
    "    \"\"\"\n",
    "    routes_graph = routes_graph.copy()\n",
    "    \n",
    "    if sum([(wtr is None), (dow is None), (daytime is None)]) != 2:\n",
    "        raise TypeError('This functions builds the graph according to only one filtering option, you have to pass one and only one.')\n",
    "    if wtr is not None:\n",
    "        mask = (complete_df['weather_main_post'] == wtr.capitalize())\n",
    "    elif dow is not None:\n",
    "        mask = (complete_df['day_of_week'] == dow)\n",
    "    else:\n",
    "        mask = (complete_df['time_pre_datetime'].between_time(daytime[0], daytime[1]))\n",
    "    \n",
    "    complete_df = complete_df[mask]\n",
    "    if not len(complete_df):\n",
    "        raise(ValueError('The filtering option you passed is wrong since no observation has matching fields.'))\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None\n",
    "    complete_df['elapsed'] /= complete_df['stop_distance']\n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "    \n",
    "    complete_df = complete_df[['elapsed', 'stop_id_post']].groupby('stop_id_post').mean()\n",
    "    nx.set_node_attributes(routes_graph, complete_df.to_dict('index'))\n",
    "    \n",
    "    delete_vx = [x[0] for x in routes_graph.nodes('elapsed') if x[1] is None]\n",
    "    routes_graph.remove_nodes_from(delete_vx)\n",
    "    routes_graph.remove_nodes_from(list(nx.isolates(routes_graph)))\n",
    "    return routes_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:28.496396Z",
     "start_time": "2023-09-06T15:02:28.493780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "for node in nx.isolates(init_graph):\n",
    "    print(node)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:28.501554Z",
     "start_time": "2023-09-06T15:02:28.496521Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['trip_id', 'start_time', 'start_date', 'route_id', 'stop_sequence',\n       'delay_pre', 'time_pre', 'uncertainty_pre', 'stop_id_pre',\n       'agency_id_pre', 'route_short_name_pre', 'route_long_name_pre',\n       'route_type_pre', 'route_url_pre', 'route_color_pre',\n       'route_text_color_pre', 'hourly_pre', 'temperature_pre',\n       'feels_like_pre', 'pressure_pre', 'humidity_pre', 'temp_min_pre',\n       'temp_max_pre', 'wind_speed_pre', 'wind_deg_pre', 'clouds_pre',\n       'weather_main_pre', 'weather_description_pre', 'weather_icon_pre',\n       'stop_code_pre', 'stop_name_pre', 'stop_desc_pre', 'stop_lat_pre',\n       'stop_lon_pre', 'stop_url_pre', 'wheelchair_boarding_pre',\n       'stop_timezone_pre', 'location_type_pre', 'parent_station_pre',\n       'delay_post', 'time_post', 'uncertainty_post', 'stop_id_post',\n       'agency_id_post', 'route_short_name_post', 'route_long_name_post',\n       'route_type_post', 'route_url_post', 'route_color_post',\n       'route_text_color_post', 'hourly_post', 'temperature_post',\n       'feels_like_post', 'pressure_post', 'humidity_post', 'temp_min_post',\n       'temp_max_post', 'wind_speed_post', 'wind_deg_post', 'clouds_post',\n       'weather_main_post', 'weather_description_post', 'weather_icon_post',\n       'stop_code_post', 'stop_name_post', 'stop_desc_post', 'stop_lat_post',\n       'stop_lon_post', 'stop_url_post', 'wheelchair_boarding_post',\n       'stop_timezone_post', 'location_type_post', 'parent_station_post',\n       'time_pre_datetime', 'day_of_week', 'stop_distance', 'elapsed'],\n      dtype='object')"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_live.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:28.514799Z",
     "start_time": "2023-09-06T15:02:28.501969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "test_graph = vertex_signal(trip_live, init_graph, wtr = 'clear')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:31.740153Z",
     "start_time": "2023-09-06T15:02:28.941382Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trend filtering test, piecewise linear case ($D = \\Delta^{(2)})$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having built the graph, we define the function producing the difference operator from the graph according to [Tibshirani R. et al. (2015)](https://jmlr.org/papers/volume17/15-147/15-147.pdf)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import scipy\n",
    "def differential_op(graph: nx.Graph, order: int) -> scipy.sparse.csr_array:\n",
    "    \"\"\"\n",
    "    Produces a linear difference operator for graph trend filtering according to Tibshirani R. et al. (2015).\n",
    "    :param graph: The graph from which to build the difference linear operator.\n",
    "    :param order: The order of the difference operator.\n",
    "    :return: The difference operator as a SciPy sparse row matrix.\n",
    "    \"\"\"\n",
    "    if order == 1:\n",
    "        out = nx.incidence_matrix(graph, oriented=True)\n",
    "    elif order == 2:\n",
    "        out = nx.laplacian_matrix(graph)\n",
    "    elif not order % 2:\n",
    "        out = scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**(order/2)\n",
    "    else:\n",
    "        out = (nx.incidence_matrix(graph, oriented=True) @ \n",
    "               scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**((order-1)/2))\n",
    "    \n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:31.745023Z",
     "start_time": "2023-09-06T15:02:31.740940Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, [remember](https://jmlr.org/papers/volume17/15-147/15-147.pdf) that what we are aiming at is a non-parametric regression in a Generalized Lasso problem fashion:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\beta} & \\quad \\frac{1}{2} \\lVert Y - \\beta \\rVert_2^2 + \\lambda \\lVert D \\beta \\rVert_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Where $D$ is an arbitrary difference (linear) operator specifying the signal structure we are enforcing through L1 penalization. The first term of the loss function is strictly convex in $\\beta$, while the second term is convex in $\\beta$ being a composition of a convex mapping and a linear mapping in $\\beta$: therefore the problem is (strictly) convex and has a (unique) solution, being the loss coercive in $\\beta$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "difference_op = differential_op(test_graph, 2)\n",
    "vector_time = np.array([x[1] for x in test_graph.nodes(data = 'elapsed')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:31.760063Z",
     "start_time": "2023-09-06T15:02:31.754625Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to get the solution out, since this is a convex problem, we use CVXPY with the CVXOPT solver, and more info can be found [here](https://github.com/elsonidoq/py-l1tf/blob/master/l1tf/impl.py)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "vlambda = 0.1 # Choosing the regularization hyperparameter\n",
    "x = cp.Variable(shape=len(vector_time)) # Variable\n",
    "obj = cp.Minimize((1/2) * cp.sum_squares(vector_time - x)\n",
    "                  + vlambda * cp.norm(difference_op @ x, 1) ) # defining the optimization problem\n",
    "prob = cp.Problem(obj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:02:31.764537Z",
     "start_time": "2023-09-06T15:02:31.761277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.2                                    \n",
      "===============================================================================\n",
      "(CVXPY) Sep 06 05:04:50 PM: Your problem has 5539 variables, 0 constraints, and 0 parameters.\n",
      "(CVXPY) Sep 06 05:04:50 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Sep 06 05:04:50 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Sep 06 05:04:50 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 06 05:04:50 PM: Compiling problem (target solver=CVXOPT).\n",
      "(CVXPY) Sep 06 05:04:50 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> CVXOPT\n",
      "(CVXPY) Sep 06 05:04:50 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Sep 06 05:04:50 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Sep 06 05:04:50 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Sep 06 05:04:50 PM: Applying reduction CVXOPT\n",
      "(CVXPY) Sep 06 05:04:50 PM: Finished problem compilation (took 3.434e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 06 05:04:50 PM: Invoking solver CVXOPT  to obtain a solution.\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -1.0000e+00  2e+06  6e+01  2e+01  1e+00\n",
      " 1:  5.9689e+02  5.9691e+02  2e+06  7e+01  2e+01  2e+00\n",
      " 2:  2.9672e+01  1.4031e+02  2e+06  7e+01  2e+01  1e+02\n",
      " 3: -1.5923e+03  1.8792e+02  1e+05  1e+01  5e+00  2e+03\n",
      " 4: -1.3399e+03  3.2449e+02  2e+04  7e+00  2e+00  2e+03\n",
      " 5: -1.0923e+03  3.5596e+02  1e+04  5e+00  2e+00  1e+03\n",
      " 6: -5.9687e+02  3.7863e+02  5e+03  3e+00  9e-01  1e+03\n",
      " 7: -2.5458e+02  4.0167e+02  3e+03  2e+00  6e-01  7e+02\n",
      " 8:  3.5849e+01  4.2251e+02  1e+03  1e+00  4e-01  4e+02\n",
      " 9:  2.4571e+02  4.4970e+02  9e+02  6e-01  2e-01  2e+02\n",
      "10:  3.5351e+02  4.6392e+02  6e+02  4e-01  1e-01  1e+02\n",
      "11:  4.2896e+02  4.7711e+02  3e+02  2e-01  6e-02  5e+01\n",
      "12:  4.6484e+02  4.8629e+02  1e+02  1e-01  4e-02  2e+01\n",
      "13:  4.8908e+02  4.9489e+02  5e+01  3e-02  1e-02  6e+00\n",
      "14:  4.9802e+02  4.9943e+02  1e+01  9e-03  3e-03  1e+00\n",
      "15:  5.0075e+02  5.0099e+02  4e+00  3e-03  1e-03  2e-01\n",
      "16:  5.0148e+02  5.0152e+02  1e+00  6e-04  2e-04  4e-02\n",
      "17:  5.0161e+02  5.0162e+02  3e-01  2e-04  7e-05  8e-03\n",
      "18:  5.0165e+02  5.0165e+02  1e-01  6e-05  2e-05  2e-03\n",
      "19:  5.0167e+02  5.0167e+02  1e-02  8e-06  3e-06  1e-04\n",
      "20:  5.0167e+02  5.0167e+02  3e-03  2e-06  5e-07  2e-05\n",
      "21:  5.0167e+02  5.0167e+02  9e-04  5e-07  2e-07  4e-06\n",
      "22:  5.0167e+02  5.0167e+02  2e-04  1e-07  4e-08  6e-07\n",
      "23:  5.0167e+02  5.0167e+02  2e-05  1e-08  5e-09  6e-08\n",
      "Optimal solution found.\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 06 05:09:40 PM: Problem status: optimal\n",
      "(CVXPY) Sep 06 05:09:40 PM: Optimal value: 5.017e+02\n",
      "(CVXPY) Sep 06 05:09:40 PM: Compilation took 3.434e-02 seconds\n",
      "(CVXPY) Sep 06 05:09:40 PM: Solver (including time spent in interface) took 2.901e+02 seconds\n",
      "Solver status: optimal\n"
     ]
    }
   ],
   "source": [
    "prob.solve(solver = cp.CVXOPT, verbose = True)\n",
    "print('Solver status: {}'.format(prob.status))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:09:40.662315Z",
     "start_time": "2023-09-06T15:04:50.552653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.32706522, 0.36752085, 0.25811697, ..., 0.4977334 , 0.34343274,\n       0.23642656])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:12:31.904889Z",
     "start_time": "2023-09-06T15:12:31.896076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "congestion_dict = dict(zip(test_graph.nodes, x.value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:12:37.175516Z",
     "start_time": "2023-09-06T15:12:37.172192Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the trend-filtering mess\n",
    "\n",
    "Our idea to validate the model in order to get a good $\\lambda$ is the following:\n",
    "- First of all we split the data in a train and a validation set, according to the time axis. The holdout set consists of 8 days, being the 20% of the overall data. `start_time` and `start_date` are (incredibly) not reliable, due to curious specification of entity attributes by Google and creative way of ATAC of intending scheduling according to the day of the week, resulting in starting times being past midnight. This forces us to use the timestamp in order to understand the exact date of a specific trip. Therefore, we convert midnight of the June 9th to Unix time, according to our time zone (GMT+2, _ora legale_)\n",
    "- Once this is done, we perform trend filtering on the graph built with a single filter specification, with a specific choice of $\\lambda$. What we get back is a dictionary, mapping each stop to its congestion value, the signal we are modelling defined as above.\n",
    "- Now we take the holdout set, and we filter it according to the same filter we have used in order to build the graph to be trend filtered. \n",
    "- For each resulting row, we consider the arrival stop and the distance between the stops. We multiply the distance by the estimated congestion signal of the stop. Then we take the difference between the estimated elapsed time and the actual (forecasted...) elapsed time. Then we take the average of the absolute values of that quantity, which is our validation metric.\n",
    "- We end up with a different chosen $\\lambda$ for each filtering option. This makes sense, since the assumption that a unique $\\lambda$ would work for all filtering options cannot hold; think about intervals of time when it rains, they are of course sparser than sunny or cloudy time periods, and it is thus more than reasonable that the information estimated when it rains is generally less representative. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "'20230617'"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_live['start_date'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:39.857272Z",
     "start_time": "2023-09-06T16:27:39.850390Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Package import\n",
    "\n",
    "First of all, we import all the necessary libraries, apart from the fundamental ones, there are some additional ones:\n",
    "\n",
    "- `cvxpy` for (convex) optimization, which is needed for trend filtering, which is a [Generalized Lasso problem](https://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf).\n",
    "- `folium` is a package useful to build the map of lines and stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import folium\n",
    "from typing import Optional"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:13.166370Z",
     "start_time": "2023-08-30T16:16:12.158554Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The static data\n",
    "- `stop_times` is a dataframe with stops (with related `stop_id`) for every and each `trip_id` and related `stop_sequence` number. It also has the cumulative distance traveled by the bus/subway/tram up until the specific stop (along the _shape_ describing the trip). \n",
    "- `trip_info` is a dataframe keeping the association between `route_id` and `trip_id`.\n",
    "- `stop_list` is a dataframe listing the stops. It is useful since it has latitude and longitude of every stop.\n",
    "- `routes` is a dataframe linking each route to its agency (ATAC, TPL or even Trenitalia) and to its type (bus, subway or tram).\n",
    "\n",
    "The `low_memory` option here is important, you don't want to have mixed type columns when performing joins ;)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "stop_times = pd.read_csv('data/static/stop_times.txt', low_memory=False)\n",
    "trip_info = pd.read_csv('data/static/trips.txt', low_memory=False)\n",
    "stop_list = pd.read_csv('data/static/stops.txt', low_memory=False)\n",
    "routes = pd.read_csv('data/static/routes.txt', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:16.242550Z",
     "start_time": "2023-08-30T16:16:13.164529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graphs and maps\n",
    "\n",
    "## The first sequence of (inner) joins\n",
    "\n",
    "First of all, we perform the first inner join (of many) between the `trip_info` table and the `stop_times` one on `trip_id`; we want to put together information related to trips and stop sequences. In other words, this way we are recovering the sequence of stops for each trip, even distinguishing between directions with the `direction_id`, which is just a binary field. The `stop_sequence` integer is the variable allowing the sequence recovering.\n",
    "\n",
    "### What's a trip? \n",
    "\n",
    "Everything else is more or less clear, but we want to stress the difference between a _route_ and a _trip_ according to the [GTFS standard](https://developers.google.com/transit/gtfs/reference?hl=en). Specifically: \n",
    "> A trip is a sequence of two or more stops that occur during a specific time period.\n",
    "\n",
    "While: \n",
    "> A route is a group of trips that are displayed to riders as a single service.\n",
    "\n",
    "Therefore $\\text{trip} \\in \\text{route}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "complete = trip_info.merge(stop_times, how = 'inner', on = 'trip_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:17.055302Z",
     "start_time": "2023-08-30T16:16:16.243781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['route_id', 'service_id', 'trip_id', 'trip_headsign', 'trip_short_name',\n       'direction_id', 'block_id', 'shape_id', 'wheelchair_accessible',\n       'exceptional', 'arrival_time', 'departure_time', 'stop_id',\n       'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type',\n       'shape_dist_traveled', 'timepoint'],\n      dtype='object')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:17.059801Z",
     "start_time": "2023-08-30T16:16:17.057090Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We drop everything that it is not needed for this stage. We keep `shape_id` since we want to use shapes later on. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "complete = complete[['route_id', 'trip_id', 'stop_id', 'stop_sequence', 'direction_id', 'shape_id']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:17.261064Z",
     "start_time": "2023-08-30T16:16:17.062191Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next inner join is to add route specific information (not trip specific). A route is what we call a _bus line_. This way we can filter out everything which is not handled by ATAC and subway/tram lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "complete = complete.merge(routes, on = 'route_id', how = 'inner')\n",
    "complete = complete[(complete['agency_id'] == 'OP1')&(complete['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:18.426278Z",
     "start_time": "2023-08-30T16:16:17.375810Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the next inner join we are recovering stop related information, like name, latitude and longitude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "complete = complete.merge(stop_list, how = 'inner', on = 'stop_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:19.387218Z",
     "start_time": "2023-08-30T16:16:18.522266Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we do not need `trip id`, we remove the column and drop the duplicates w.r.t. route, stop sequence and direction. At this stage, we only want to build (and plot) an undirected graph of ATAC public transport relying on buses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "complete = (complete.drop('trip_id', axis = 1).drop_duplicates(['route_id', 'stop_sequence', 'direction_id']).reset_index())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.113454Z",
     "start_time": "2023-08-30T16:16:19.388532Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having initialized the graph, we add nodes by simply reading the `stop_list` dataframe line by line. Then, we add edges by sorting by `stop_sequence` and by grouping by `route_id` and `direction_id` (`groupby` in Pandas preserves row ordering). Once done that, we can just iterate group by group, adding edges in a sequential order if not already present in the graph. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "init_graph = nx.Graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.117628Z",
     "start_time": "2023-08-30T16:16:20.114113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for _, row in stop_list.iterrows():\n",
    "    init_graph.add_node(row['stop_id'], name = row['stop_name'], latitude = row['stop_lat'], longitude = row['stop_lon'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.296353Z",
     "start_time": "2023-08-30T16:16:20.116633Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "routes_grouped = complete.sort_values(by='stop_sequence').groupby(['route_id', 'direction_id'])\n",
    "for _, group in routes_grouped:\n",
    "    stops = group['stop_id'].tolist()\n",
    "    edges = [(stops[i], stops[i+1]) for i in range(len(stops)-1) if not init_graph.has_edge(stops[i], stops[i+1])]\n",
    "    init_graph.add_edges_from(edges)\n",
    "    \n",
    "init_graph.remove_nodes_from(list(nx.isolates(init_graph))) # if present, we remove isolated nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.325901Z",
     "start_time": "2023-08-30T16:16:20.297799Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the graph over the map (with Folium)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are plotting the edges (not the vertexes, to avoid overplotting) with Folium; the map is centered in the Campidoglio (XD), with a moderate zoom. To do this, we are adding the edges through `Polyline` objects added to the baseline _terrain layer_ map. The map is then saved locally and can be opened with a browser. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "\n",
    "# for _, row in stop_list.iterrows():\n",
    "#     if init_graph.has_node(row['stop_id']):\n",
    "#         folium.Marker(\n",
    "#             location=[row['stop_lat'], row['stop_lon']],\n",
    "#             popup=f\"{row['stop_name']}\",\n",
    "#             icon = folium.Icon(prefix='fa', icon = 'bus', icon_size=(5, 5))\n",
    "#         ).add_to(rome)\n",
    "\n",
    "coords = [(edge[0], edge[1]) for edge in init_graph.edges()]\n",
    "for _, group in routes_grouped:\n",
    "    coords = [(lat, long) for lat, long in zip(group['stop_lat'], group['stop_lon'])]\n",
    "    folium.PolyLine(locations=coords, color='blue', weight=1, opacity=0.5).add_to(rome)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.394632Z",
     "start_time": "2023-08-30T16:16:20.325733Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "rome.save('maps/test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.525439Z",
     "start_time": "2023-08-30T16:16:20.395180Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An improvement: use _shape_ data\n",
    "\n",
    "Instead of relying on latitude and longitude in order to visualize things, we use what Google Maps actually uses, AKA [_shape_](https://developers.google.com/transit/gtfs/reference?hl=en#shapestxt) data. This is not difficult to do, since we have a `shape.txt` file storing that information.\n",
    "\n",
    "This what we could when developing a possible application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "shapes = pd.read_csv('data/static/shapes.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.636157Z",
     "start_time": "2023-08-30T16:16:20.528432Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We remove the shapes of what does not belong to our target data (ATAC + buses)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "shapes = shapes[shapes['shape_id'].isin(complete['shape_id'])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:20.658016Z",
     "start_time": "2023-08-30T16:16:20.644875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "grouped = shapes.sort_values(by = 'shape_pt_sequence').groupby('shape_id')\n",
    "for _, group in grouped:\n",
    "    coords = [(lat, lon) for lat, lon in zip(group['shape_pt_lat'], group['shape_pt_lon'])]\n",
    "    folium.PolyLine(locations=coords, color = 'blue', weight = 1, opacity = 0.5).add_to(rome)\n",
    "    \n",
    "rome.save('maps/improved_test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:22.052946Z",
     "start_time": "2023-08-30T16:16:20.656418Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling live data\n",
    "\n",
    "Now, we read the real time data, the scraped one, if scraping can be the right wording for it, since it was just GET calls to the Rome GTFS feed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "trip_live = pd.read_feather('data/trip-updates.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:24.081667Z",
     "start_time": "2023-08-30T16:16:22.053718Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 0 problem: the first stop of every trip has 0 as its timestamp: we need to remove those rows since they are basically useless."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "trip_live = trip_live[trip_live['time'] != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:25.252657Z",
     "start_time": "2023-08-30T16:16:24.081829Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again , we consider only the bus lines belonging to ATAC, and only bus lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(routes, on = 'route_id', how = 'inner')\n",
    "trip_live = trip_live[(trip_live['agency_id'] == 'OP1')&(trip_live['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:32.369205Z",
     "start_time": "2023-08-30T16:16:25.803548Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add weather"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add the weather data, collected with the [Open Weather history API](https://openweathermap.org/history)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "weather_main\nClouds          461\nClear           414\nRain             98\nThunderstorm     24\nMist              6\nDrizzle           3\nFog               2\nName: count, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_feather('data/weather_df.feather')\n",
    "weather['weather_main'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:32.376054Z",
     "start_time": "2023-08-30T16:16:32.369956Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weather data is hourly, so we use floor division to move from the UNIX timestamp (in seconds) to the integer hours. This is the case for both real time data and weather. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "trip_live['hourly'] = trip_live['time'] // 3600\n",
    "weather['hourly'] = weather['timestamp'] // 3600\n",
    "weather.drop('timestamp', axis = 1, inplace=True)\n",
    "trip_live = trip_live.merge(weather, how='inner', on='hourly')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:36.717555Z",
     "start_time": "2023-08-30T16:16:32.377285Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add stop info\n",
    "\n",
    "Now we add stop info, both for what concerns stop specific information and for what concerns the stop sequence. With `stop_times` we also get the [travelled distance in-between stops](https://developers.google.com/transit/gtfs/reference?hl=en#stop_timestxt). The problem is that the foreign key for `stop_times` is trip-related, and static data on trips is continuously updated (once a day); therefore, we cannot really do an inner join based on `trip_id`, there would be a lot of trips not found and removing the suffix in the ID is not an option, since we would miss the edge direction. \n",
    "\n",
    "Therefore, we need to be creative. This is similar to a trick we are using also later on, first of all, we group together observations in rows representing disjoint groups of pairs. Each row gathers `stop_time` information from the departure and arrival stop. As we have already said, our focus for `stop_times` is the (real) travelled distance between subsequent stops, but we assume that this is invariant across routes. A bus takes the same path to go from stop A to B, independently of the fact that the path belongs to a route or another. \n",
    "\n",
    "Therefore, we can just keep departure and arrival `stop_id`s, and then take the difference between the cumulative distance in order to get the travelled distance between each (ordered) pair of stops. We will then perform an inner join based on the `stop_id`s."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(stop_list, on = 'stop_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:46.983710Z",
     "start_time": "2023-08-30T16:16:37.187844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "route_stop = trip_live[['trip_id', 'route_id', 'stop_sequence']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:47.236832Z",
     "start_time": "2023-08-30T16:16:46.984493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "route_stop = route_stop.merge(stop_times, on = ['trip_id', 'stop_sequence'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:16:50.040442Z",
     "start_time": "2023-08-30T16:16:47.238045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "route_stop_shifted = route_stop.copy()\n",
    "route_stop_shifted['stop_sequence'] = route_stop_shifted['stop_sequence']-1\n",
    "route_stop = route_stop.merge(route_stop_shifted, on = ['trip_id', 'stop_sequence'], suffixes=('_pre', '_post'))\n",
    "route_stop['stop_distance'] = route_stop['shape_dist_traveled_post']-route_stop['shape_dist_traveled_pre']\n",
    "del route_stop_shifted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:17:05.692276Z",
     "start_time": "2023-08-30T16:16:50.041126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "route_stop = route_stop[['stop_id_pre', 'stop_id_post', 'stop_distance']].drop_duplicates(subset=['stop_id_pre', 'stop_id_post'], ignore_index=True, keep = 'first')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:17:11.454085Z",
     "start_time": "2023-08-30T16:17:05.691317Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Put together sequential updates\n",
    "\n",
    "Now we put together information related to sequential updates, each (disjoint) pair of records/stops in a trip is put together."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "trip_live = trip_live.drop_duplicates(subset=['trip_id', 'start_time', 'start_date', 'stop_sequence'], keep = 'first', ignore_index=True).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:17:26.044860Z",
     "start_time": "2023-08-30T16:17:11.598207Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The trick is copying the dataset and then shifting the `stop_sequence` by -1. At this point, an inner join between the two dataset (the copied and the original one) is enough. Suffixes `_pre` and `_post` are enough to distinguish the two sets of fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "trip_live_shifted = trip_live.copy()\n",
    "trip_live_shifted['stop_sequence'] = trip_live_shifted['stop_sequence'] - 1\n",
    "trip_live = trip_live.merge(trip_live_shifted, how = 'inner', on = ['route_id', 'trip_id', 'start_time', 'start_date', 'stop_sequence'], suffixes=('_pre', '_post'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:17:50.545254Z",
     "start_time": "2023-08-30T16:17:26.044384Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add day of the week\n",
    "\n",
    "We add the day of the week (as integer) in order to be more specific when building the (filtered) graph. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "trip_live['time_pre_datetime'] = pd.to_datetime(trip_live['time_pre'], origin='unix', unit = 's')\n",
    "trip_live['day_of_week'] = trip_live.time_pre_datetime.dt.weekday"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:17:51.423232Z",
     "start_time": "2023-08-30T16:17:50.534241Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add distances and time\n",
    "For the distances, we take the inner join we were talking about before, using departure and arrival `stop_id`s. This is the exact distance travelled along the total [_shape_](https://developers.google.com/transit/gtfs/reference?hl=en#shapestxt) length, where the _shape_ is a collection of points describing the trip path. Then we take difference between (prediction) timestamps to get the elapsed time between stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(route_stop, on = ['stop_id_pre', 'stop_id_post'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:06.058268Z",
     "start_time": "2023-08-30T16:17:51.423811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "trip_live['elapsed'] = trip_live['time_post']-trip_live['time_pre']\n",
    "trip_live = trip_live[trip_live['elapsed'] >= 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:11.437205Z",
     "start_time": "2023-08-30T16:18:06.056920Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function to build the signal over the graph vertexes\n",
    "The following is a function to define the signal over the vertexes. The way we are defining the signal over the vertex set is the following: \n",
    "> The signal for each vertex is the average value (averaging along the observations gathered through some filtering options) of the elapsed time to get to a specific vertex from the previous divided by the (shape) distance between the previous vertex to the next.\n",
    "\n",
    "The resulting graph is thus related only to a specific filtering option (based on weather, day of the week and time of the day). We allow the choice of only one filtering option, in order to have enough observations to estimate meaningful average of the signal instances. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def vertex_signal(complete_df: pd.DataFrame, routes_graph: nx.Graph, *, wtr: Optional[str] = None, dow: Optional[int] = None, daytime:Optional[tuple[int, int]] = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Function assigning signal over the public transport graph vertexes by averaging across the inbound edges \n",
    "    elapsed time, according to a specific filtering option, passed as keyword argument.\n",
    "    :param complete_df: The dataframe containing the preprocessed data.\n",
    "    :param routes_graph: The graph, already built.\n",
    "    :param wtr: Main weather conditions.\n",
    "    :param dow: The day of the week, as integer.\n",
    "    :param daytime: The daytime, as an interval specified by a tuple of two integers.\n",
    "    :return: The graph with the signal defined over the vertex set.\n",
    "    \"\"\"\n",
    "    routes_graph = routes_graph.copy()\n",
    "    \n",
    "    if sum([(wtr is None), (dow is None), (daytime is None)]) != 2:\n",
    "        raise TypeError('This functions builds the graph according to only one filtering option, you have to pass one and only one.')\n",
    "    if wtr is not None:\n",
    "        mask = (complete_df['weather_main_post'] == wtr.capitalize())\n",
    "    elif dow is not None:\n",
    "        mask = (complete_df['day_of_week'] == dow)\n",
    "    else:\n",
    "        mask = (complete_df['time_pre_datetime'].between_time(daytime[0], daytime[1]))\n",
    "    \n",
    "    complete_df = complete_df[mask]\n",
    "    if not len(complete_df):\n",
    "        raise(ValueError('The filtering option you passed is wrong since no observation has matching fields.'))\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None\n",
    "    complete_df['elapsed'] /= complete_df['stop_distance']\n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "    \n",
    "    complete_df = complete_df[['elapsed', 'stop_id_post']].groupby('stop_id_post').mean()\n",
    "    nx.set_node_attributes(routes_graph, complete_df.to_dict('index'))\n",
    "    \n",
    "    delete_vx = [x[0] for x in routes_graph.nodes('elapsed') if x[1] is None]\n",
    "    routes_graph.remove_nodes_from(delete_vx)\n",
    "    routes_graph.remove_nodes_from(list(nx.isolates(routes_graph)))\n",
    "    return routes_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:11.452111Z",
     "start_time": "2023-08-30T16:18:11.450288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "for node in nx.isolates(init_graph):\n",
    "    print(node)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:11.460252Z",
     "start_time": "2023-08-30T16:18:11.452177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['trip_id', 'start_time', 'start_date', 'route_id', 'stop_sequence',\n       'delay_pre', 'time_pre', 'uncertainty_pre', 'stop_id_pre',\n       'agency_id_pre', 'route_short_name_pre', 'route_long_name_pre',\n       'route_type_pre', 'route_url_pre', 'route_color_pre',\n       'route_text_color_pre', 'hourly_pre', 'temperature_pre',\n       'feels_like_pre', 'pressure_pre', 'humidity_pre', 'temp_min_pre',\n       'temp_max_pre', 'wind_speed_pre', 'wind_deg_pre', 'clouds_pre',\n       'weather_main_pre', 'weather_description_pre', 'weather_icon_pre',\n       'stop_code_pre', 'stop_name_pre', 'stop_desc_pre', 'stop_lat_pre',\n       'stop_lon_pre', 'stop_url_pre', 'wheelchair_boarding_pre',\n       'stop_timezone_pre', 'location_type_pre', 'parent_station_pre',\n       'delay_post', 'time_post', 'uncertainty_post', 'stop_id_post',\n       'agency_id_post', 'route_short_name_post', 'route_long_name_post',\n       'route_type_post', 'route_url_post', 'route_color_post',\n       'route_text_color_post', 'hourly_post', 'temperature_post',\n       'feels_like_post', 'pressure_post', 'humidity_post', 'temp_min_post',\n       'temp_max_post', 'wind_speed_post', 'wind_deg_post', 'clouds_post',\n       'weather_main_post', 'weather_description_post', 'weather_icon_post',\n       'stop_code_post', 'stop_name_post', 'stop_desc_post', 'stop_lat_post',\n       'stop_lon_post', 'stop_url_post', 'wheelchair_boarding_post',\n       'stop_timezone_post', 'location_type_post', 'parent_station_post',\n       'time_pre_datetime', 'day_of_week', 'stop_distance', 'elapsed'],\n      dtype='object')"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_live.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:11.469048Z",
     "start_time": "2023-08-30T16:18:11.458091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "test_graph = vertex_signal(trip_live, init_graph, wtr = 'clear')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:14.049592Z",
     "start_time": "2023-08-30T16:18:11.905737Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trend filtering test, piecewise linear case ($D = \\Delta^{(2)})$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having built the graph, we define the function producing the difference operator from the graph according to [Tibshirani R. et al. (2015)](https://jmlr.org/papers/volume17/15-147/15-147.pdf)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import scipy\n",
    "def differential_op(graph: nx.Graph, order: int) -> scipy.sparse.csr_array:\n",
    "    \"\"\"\n",
    "    Produces a linear difference operator for graph trend filtering according to Tibshirani R. et al. (2015).\n",
    "    :param graph: The graph from which to build the difference linear operator.\n",
    "    :param order: The order of the difference operator.\n",
    "    :return: The difference operator as a SciPy sparse row matrix.\n",
    "    \"\"\"\n",
    "    if order == 1:\n",
    "        out = nx.incidence_matrix(graph, oriented=True)\n",
    "    elif order == 2:\n",
    "        out = nx.laplacian_matrix(graph)\n",
    "    elif not order % 2:\n",
    "        out = scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**(order/2)\n",
    "    else:\n",
    "        out = (nx.incidence_matrix(graph, oriented=True) @ \n",
    "               scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**((order-1)/2))\n",
    "    \n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:14.057220Z",
     "start_time": "2023-08-30T16:18:14.050887Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, [remember](https://jmlr.org/papers/volume17/15-147/15-147.pdf) that what we are aiming at is a non-parametric regression in a Generalized Lasso problem fashion:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\beta} & \\quad \\frac{1}{2} \\lVert Y - \\beta \\rVert_2^2 + \\lambda \\lVert D \\beta \\rVert_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Where $D$ is an arbitrary difference (linear) operator specifying the signal structure we are enforcing through L1 penalization. The first term of the loss function is strictly convex in $\\beta$, while the second term is convex in $\\beta$ being a composition of a convex mapping and a linear mapping in $\\beta$: therefore the problem is (strictly) convex and has a (unique) solution, being the loss coercive in $\\beta$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "difference_op = differential_op(test_graph, 2)\n",
    "vector_time = np.array([x[1] for x in test_graph.nodes(data = 'elapsed')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:14.069507Z",
     "start_time": "2023-08-30T16:18:14.065574Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to get the solution out, since this is a convex problem, we use CVXPY with the CVXOPT solver, and more info can be found [here](https://github.com/elsonidoq/py-l1tf/blob/master/l1tf/impl.py)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "vlambda = 0.1 # Choosing the regularization hyperparameter\n",
    "x = cp.Variable(shape=len(vector_time)) # Variable\n",
    "obj = cp.Minimize((1/2) * cp.sum_squares(vector_time - x)\n",
    "                  + vlambda * cp.norm(difference_op @ x, 1) ) # defining the optimization problem\n",
    "prob = cp.Problem(obj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:18:14.088042Z",
     "start_time": "2023-08-30T16:18:14.071784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.2                                    \n",
      "===============================================================================\n",
      "(CVXPY) Aug 30 06:18:14 PM: Your problem has 5539 variables, 0 constraints, and 0 parameters.\n",
      "(CVXPY) Aug 30 06:18:14 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Aug 30 06:18:14 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Aug 30 06:18:14 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 30 06:18:14 PM: Compiling problem (target solver=CVXOPT).\n",
      "(CVXPY) Aug 30 06:18:14 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> CVXOPT\n",
      "(CVXPY) Aug 30 06:18:14 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Aug 30 06:18:14 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Aug 30 06:18:14 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Aug 30 06:18:14 PM: Applying reduction CVXOPT\n",
      "(CVXPY) Aug 30 06:18:14 PM: Finished problem compilation (took 2.451e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 30 06:18:14 PM: Invoking solver CVXOPT  to obtain a solution.\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -1.0000e+00  2e+06  6e+01  2e+01  1e+00\n",
      " 1:  6.1463e+02  6.1460e+02  2e+06  7e+01  2e+01  2e+00\n",
      " 2:  6.2061e+01  1.5699e+02  2e+06  7e+01  2e+01  1e+02\n",
      " 3: -1.5101e+03  2.2393e+02  2e+05  2e+01  6e+00  2e+03\n",
      " 4: -1.6377e+03  3.4202e+02  3e+04  8e+00  3e+00  2e+03\n",
      " 5: -1.3303e+03  3.7531e+02  1e+04  5e+00  2e+00  2e+03\n",
      " 6: -8.2038e+02  3.9314e+02  6e+03  3e+00  1e+00  1e+03\n",
      " 7: -3.3754e+02  4.1339e+02  3e+03  2e+00  6e-01  8e+02\n",
      " 8: -2.4835e+01  4.3282e+02  2e+03  1e+00  4e-01  5e+02\n",
      " 9:  2.2077e+02  4.6260e+02  1e+03  7e-01  2e-01  2e+02\n",
      "10:  3.7602e+02  4.8093e+02  7e+02  4e-01  1e-01  1e+02\n",
      "11:  4.3321e+02  4.8871e+02  4e+02  3e-01  9e-02  6e+01\n",
      "12:  4.8044e+02  4.9654e+02  3e+02  2e-01  6e-02  2e+01\n",
      "13:  5.0041e+02  5.0403e+02  6e+01  4e-02  1e-02  4e+00\n",
      "14:  5.0819e+02  5.0899e+02  2e+01  1e-02  4e-03  8e-01\n",
      "15:  5.1093e+02  5.1114e+02  6e+00  4e-03  1e-03  2e-01\n",
      "16:  5.1179e+02  5.1183e+02  2e+00  1e-03  3e-04  4e-02\n",
      "17:  5.1192e+02  5.1193e+02  7e-01  5e-04  2e-04  1e-02\n",
      "18:  5.1201e+02  5.1201e+02  4e-01  2e-04  7e-05  3e-03\n",
      "19:  5.1205e+02  5.1205e+02  1e-01  6e-05  2e-05  6e-04\n",
      "20:  5.1206e+02  5.1206e+02  3e-02  2e-05  7e-06  1e-04\n",
      "21:  5.1207e+02  5.1207e+02  7e-03  4e-06  1e-06  2e-05\n",
      "22:  5.1207e+02  5.1207e+02  5e-04  3e-07  9e-08  7e-07\n",
      "23:  5.1207e+02  5.1207e+02  5e-05  3e-08  1e-08  7e-08\n",
      "Optimal solution found.\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 30 06:22:48 PM: Problem status: optimal\n",
      "(CVXPY) Aug 30 06:22:48 PM: Optimal value: 5.121e+02\n",
      "(CVXPY) Aug 30 06:22:48 PM: Compilation took 2.451e-02 seconds\n",
      "(CVXPY) Aug 30 06:22:48 PM: Solver (including time spent in interface) took 2.740e+02 seconds\n",
      "Solver status: optimal\n"
     ]
    }
   ],
   "source": [
    "prob.solve(solver = cp.CVXOPT, verbose = True)\n",
    "print('Solver status: {}'.format(prob.status))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:22:48.129650Z",
     "start_time": "2023-08-30T16:18:14.074546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.32933342, 0.42835848, 0.2578881 , ..., 0.29834682, 0.28290484,\n       0.22557054])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:30:08.896027Z",
     "start_time": "2023-08-30T16:30:08.890065Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

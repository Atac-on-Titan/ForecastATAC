{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Package import\n",
    "\n",
    "First of all, we import all the necessary libraries, apart from the fundamental ones, there are some additional ones:\n",
    "\n",
    "- `cvxpy` for (convex) optimization, which is needed for trend filtering, which is a [Generalized Lasso problem](https://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf).\n",
    "- `folium` is a package useful to build the map of lines and stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import folium\n",
    "from typing import Optional\n",
    "import pytz"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:26.347439Z",
     "start_time": "2023-09-07T22:04:26.171392Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The static data\n",
    "- `stop_times` is a dataframe with stops (with related `stop_id`) for every and each `trip_id` and related `stop_sequence` number. It also has the cumulative distance traveled by the bus/subway/tram up until the specific stop (along the _shape_ describing the trip). \n",
    "- `trip_info` is a dataframe keeping the association between `route_id` and `trip_id`.\n",
    "- `stop_list` is a dataframe listing the stops. It is useful since it has latitude and longitude of every stop.\n",
    "- `routes` is a dataframe linking each route to its agency (ATAC, TPL or even Trenitalia) and to its type (bus, subway or tram).\n",
    "\n",
    "The `low_memory` option here is important, you don't want to have mixed type columns when performing joins ;)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "stop_times = pd.read_csv('data/static/stop_times.txt', low_memory=False)\n",
    "trip_info = pd.read_csv('data/static/trips.txt', low_memory=False)\n",
    "stop_list = pd.read_csv('data/static/stops.txt', low_memory=False)\n",
    "routes = pd.read_csv('data/static/routes.txt', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:29.457331Z",
     "start_time": "2023-09-07T22:04:26.177628Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graphs and maps\n",
    "\n",
    "## The first sequence of (inner) joins\n",
    "\n",
    "First of all, we perform the first inner join (of many) between the `trip_info` table and the `stop_times` one on `trip_id`; we want to put together information related to trips and stop sequences. In other words, this way we are recovering the sequence of stops for each trip, even distinguishing between directions with the `direction_id`, which is just a binary field. The `stop_sequence` integer is the variable allowing the sequence recovering.\n",
    "\n",
    "### What's a trip? \n",
    "\n",
    "Everything else is more or less clear, but we want to stress the difference between a _route_ and a _trip_ according to the [GTFS standard](https://developers.google.com/transit/gtfs/reference?hl=en). Specifically: \n",
    "> A trip is a sequence of two or more stops that occur during a specific time period.\n",
    "\n",
    "While: \n",
    "> A route is a group of trips that are displayed to riders as a single service.\n",
    "\n",
    "Therefore $\\text{trip} \\in \\text{route}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "complete = trip_info.merge(stop_times, how = 'inner', on = 'trip_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:30.408068Z",
     "start_time": "2023-09-07T22:04:29.455613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['route_id', 'service_id', 'trip_id', 'trip_headsign', 'trip_short_name',\n       'direction_id', 'block_id', 'shape_id', 'wheelchair_accessible',\n       'exceptional', 'arrival_time', 'departure_time', 'stop_id',\n       'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type',\n       'shape_dist_traveled', 'timepoint'],\n      dtype='object')"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:30.411348Z",
     "start_time": "2023-09-07T22:04:30.408684Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We drop everything that it is not needed for this stage. We keep `shape_id` since we want to use shapes later on. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "complete = complete[['route_id', 'trip_id', 'stop_id', 'stop_sequence', 'direction_id', 'shape_id']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:30.620637Z",
     "start_time": "2023-09-07T22:04:30.412661Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next inner join is to add route specific information (not trip specific). A route is what we call a _bus line_. This way we can filter out everything which is not handled by ATAC and subway/tram lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "complete = complete.merge(routes, on = 'route_id', how = 'inner')\n",
    "complete = complete[(complete['agency_id'] == 'OP1')&(complete['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:31.867378Z",
     "start_time": "2023-09-07T22:04:30.738803Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the next inner join we are recovering stop related information, like name, latitude and longitude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "complete = complete.merge(stop_list, how = 'inner', on = 'stop_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:32.931593Z",
     "start_time": "2023-09-07T22:04:31.965646Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we do not need `trip id`, we remove the column and drop the duplicates w.r.t. route, stop sequence and direction. At this stage, we only want to build (and plot) an undirected graph of ATAC public transport relying on buses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "complete = (complete.drop('trip_id', axis = 1).drop_duplicates(['route_id', 'stop_sequence', 'direction_id']).reset_index())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:33.687891Z",
     "start_time": "2023-09-07T22:04:32.932161Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having initialized the graph, we add nodes by simply reading the `stop_list` dataframe line by line. Then, we add edges by sorting by `stop_sequence` and by grouping by `route_id` and `direction_id` (`groupby` in Pandas preserves row ordering). Once done that, we can just iterate group by group, adding edges in a sequential order if not already present in the graph. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "init_graph = nx.Graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:33.688077Z",
     "start_time": "2023-09-07T22:04:33.682951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "for _, row in stop_list.iterrows():\n",
    "    init_graph.add_node(row['stop_id'], name = row['stop_name'], latitude = row['stop_lat'], longitude = row['stop_lon'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:33.870283Z",
     "start_time": "2023-09-07T22:04:33.686618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "routes_grouped = complete.sort_values(by='stop_sequence').groupby(['route_id', 'direction_id'])\n",
    "for _, group in routes_grouped:\n",
    "    stops = group['stop_id'].tolist()\n",
    "    edges = [(stops[i], stops[i+1]) for i in range(len(stops)-1) if not init_graph.has_edge(stops[i], stops[i+1])]\n",
    "    init_graph.add_edges_from(edges)\n",
    "    \n",
    "init_graph.remove_nodes_from(list(nx.isolates(init_graph))) # if present, we remove isolated nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:33.912270Z",
     "start_time": "2023-09-07T22:04:33.873126Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the graph over the map (with Folium)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are plotting the edges (not the vertexes, to avoid overplotting) with Folium; the map is centered in the Campidoglio (XD), with a moderate zoom. To do this, we are adding the edges through `Polyline` objects added to the baseline _terrain layer_ map. The map is then saved locally and can be opened with a browser. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "\n",
    "# for _, row in stop_list.iterrows():\n",
    "#     if init_graph.has_node(row['stop_id']):\n",
    "#         folium.Marker(\n",
    "#             location=[row['stop_lat'], row['stop_lon']],\n",
    "#             popup=f\"{row['stop_name']}\",\n",
    "#             icon = folium.Icon(prefix='fa', icon = 'bus', icon_size=(5, 5))\n",
    "#         ).add_to(rome)\n",
    "\n",
    "coords = [(edge[0], edge[1]) for edge in init_graph.edges()]\n",
    "for _, group in routes_grouped:\n",
    "    coords = [(lat, long) for lat, long in zip(group['stop_lat'], group['stop_lon'])]\n",
    "    folium.PolyLine(locations=coords, color='blue', weight=1, opacity=0.5).add_to(rome)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:34.085703Z",
     "start_time": "2023-09-07T22:04:33.909017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "rome.save('maps/test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:34.230833Z",
     "start_time": "2023-09-07T22:04:34.085092Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An improvement: use _shape_ data\n",
    "\n",
    "Instead of relying on latitude and longitude in order to visualize things, we use what Google Maps actually uses, AKA [_shape_](https://developers.google.com/transit/gtfs/reference?hl=en#shapestxt) data. This is not difficult to do, since we have a `shape.txt` file storing that information.\n",
    "\n",
    "This what we could when developing a possible application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "shapes = pd.read_csv('data/static/shapes.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:34.325895Z",
     "start_time": "2023-09-07T22:04:34.215944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We remove the shapes of what does not belong to our target data (ATAC + buses)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "shapes = shapes[shapes['shape_id'].isin(complete['shape_id'])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:34.351622Z",
     "start_time": "2023-09-07T22:04:34.335843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "rome = folium.Map(location = (41.89, 12.48), zoom_start=20)\n",
    "grouped = shapes.sort_values(by = 'shape_pt_sequence').groupby('shape_id')\n",
    "for _, group in grouped:\n",
    "    coords = [(lat, lon) for lat, lon in zip(group['shape_pt_lat'], group['shape_pt_lon'])]\n",
    "    folium.PolyLine(locations=coords, color = 'blue', weight = 1, opacity = 0.5).add_to(rome)\n",
    "    \n",
    "rome.save('maps/improved_test.html')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:35.798534Z",
     "start_time": "2023-09-07T22:04:34.348431Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling live data\n",
    "\n",
    "Now, we read the real time data, the scraped one, if scraping can be the right wording for it, since it was just GET calls to the Rome GTFS feed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "trip_live = pd.read_feather('data/trip-updates.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:38.437529Z",
     "start_time": "2023-09-07T22:04:35.798756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             trip_id start_time start_date  \\\n0                                           0#785-17   16:39:00   20230506   \n1                                           0#785-17   16:39:00   20230506   \n2                                           0#785-17   16:39:00   20230506   \n3                                           0#785-17   16:39:00   20230506   \n4                                           0#785-17   16:39:00   20230506   \n...                                              ...        ...        ...   \n25968156  VJfe5102acfc4e0719a2adf07b4737b5a00084fec5   14:52:00   20230618   \n25968157  VJfe5102acfc4e0719a2adf07b4737b5a00084fec5   14:52:00   20230618   \n25968158  VJfe5102acfc4e0719a2adf07b4737b5a00084fec5   14:52:00   20230618   \n25968159  VJfe5102acfc4e0719a2adf07b4737b5a00084fec5   14:52:00   20230618   \n25968160  VJfe5102acfc4e0719a2adf07b4737b5a00084fec5   14:52:00   20230618   \n\n         route_id  stop_sequence  delay        time  uncertainty stop_id  \n0             115             12     46  1683384892           21   20323  \n1             115             13     36  1683384964           21   20215  \n2             115             14     56  1683385076           21   79756  \n3             115             15     90  1683385168           21   72993  \n4             115             16     56  1683385222           21   20295  \n...           ...            ...    ...         ...          ...     ...  \n25968156       24             20      0  1687094142            0   75863  \n25968157       24             21      0  1687094173            0   78828  \n25968158       24             22      0  1687094253            0   71460  \n25968159       24             23      0  1687094322            0   77865  \n25968160       24             24      0  1687094381            0   79307  \n\n[25968161 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trip_id</th>\n      <th>start_time</th>\n      <th>start_date</th>\n      <th>route_id</th>\n      <th>stop_sequence</th>\n      <th>delay</th>\n      <th>time</th>\n      <th>uncertainty</th>\n      <th>stop_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0#785-17</td>\n      <td>16:39:00</td>\n      <td>20230506</td>\n      <td>115</td>\n      <td>12</td>\n      <td>46</td>\n      <td>1683384892</td>\n      <td>21</td>\n      <td>20323</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0#785-17</td>\n      <td>16:39:00</td>\n      <td>20230506</td>\n      <td>115</td>\n      <td>13</td>\n      <td>36</td>\n      <td>1683384964</td>\n      <td>21</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0#785-17</td>\n      <td>16:39:00</td>\n      <td>20230506</td>\n      <td>115</td>\n      <td>14</td>\n      <td>56</td>\n      <td>1683385076</td>\n      <td>21</td>\n      <td>79756</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0#785-17</td>\n      <td>16:39:00</td>\n      <td>20230506</td>\n      <td>115</td>\n      <td>15</td>\n      <td>90</td>\n      <td>1683385168</td>\n      <td>21</td>\n      <td>72993</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0#785-17</td>\n      <td>16:39:00</td>\n      <td>20230506</td>\n      <td>115</td>\n      <td>16</td>\n      <td>56</td>\n      <td>1683385222</td>\n      <td>21</td>\n      <td>20295</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25968156</th>\n      <td>VJfe5102acfc4e0719a2adf07b4737b5a00084fec5</td>\n      <td>14:52:00</td>\n      <td>20230618</td>\n      <td>24</td>\n      <td>20</td>\n      <td>0</td>\n      <td>1687094142</td>\n      <td>0</td>\n      <td>75863</td>\n    </tr>\n    <tr>\n      <th>25968157</th>\n      <td>VJfe5102acfc4e0719a2adf07b4737b5a00084fec5</td>\n      <td>14:52:00</td>\n      <td>20230618</td>\n      <td>24</td>\n      <td>21</td>\n      <td>0</td>\n      <td>1687094173</td>\n      <td>0</td>\n      <td>78828</td>\n    </tr>\n    <tr>\n      <th>25968158</th>\n      <td>VJfe5102acfc4e0719a2adf07b4737b5a00084fec5</td>\n      <td>14:52:00</td>\n      <td>20230618</td>\n      <td>24</td>\n      <td>22</td>\n      <td>0</td>\n      <td>1687094253</td>\n      <td>0</td>\n      <td>71460</td>\n    </tr>\n    <tr>\n      <th>25968159</th>\n      <td>VJfe5102acfc4e0719a2adf07b4737b5a00084fec5</td>\n      <td>14:52:00</td>\n      <td>20230618</td>\n      <td>24</td>\n      <td>23</td>\n      <td>0</td>\n      <td>1687094322</td>\n      <td>0</td>\n      <td>77865</td>\n    </tr>\n    <tr>\n      <th>25968160</th>\n      <td>VJfe5102acfc4e0719a2adf07b4737b5a00084fec5</td>\n      <td>14:52:00</td>\n      <td>20230618</td>\n      <td>24</td>\n      <td>24</td>\n      <td>0</td>\n      <td>1687094381</td>\n      <td>0</td>\n      <td>79307</td>\n    </tr>\n  </tbody>\n</table>\n<p>25968161 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_feather('data/trip-updates.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:40.763855Z",
     "start_time": "2023-09-07T22:04:38.438223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "trip_live = trip_live.drop_duplicates(subset=['trip_id', 'start_time', 'start_date', 'stop_sequence'], keep = 'last', ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:46.670438Z",
     "start_time": "2023-09-07T22:04:40.986399Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 0 problem: the first stop of every trip has 0 as its timestamp: we need to remove those rows since they are basically useless."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "trip_live = trip_live[trip_live['time'] != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:47.619026Z",
     "start_time": "2023-09-07T22:04:46.671053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again , we consider only the bus lines belonging to ATAC, and only bus lines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(routes, on = 'route_id', how = 'inner')\n",
    "trip_live = trip_live[(trip_live['agency_id'] == 'OP1')&(trip_live['route_type'] == 3)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:54.131230Z",
     "start_time": "2023-09-07T22:04:48.056480Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add weather"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add the weather data, collected with the [Open Weather history API](https://openweathermap.org/history)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "weather_main\nClouds          461\nClear           414\nRain             98\nThunderstorm     24\nMist              6\nDrizzle           3\nFog               2\nName: count, dtype: int64"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_feather('data/weather_df.feather')\n",
    "weather['weather_main'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:54.139993Z",
     "start_time": "2023-09-07T22:04:54.131501Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We encode the weather as a binary variable where clear conditions are 0, and anything else is 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weather_to_binary(weather: str):\n",
    "    condition_to_binary = {\"Clouds\": 0, \"Clear\": 0, \"Rain\": 1, \"Drizzle\": 1, \"Mist\": 1, \"Thunderstorm\": 1, \"Fog\": 1}\n",
    "    return condition_to_binary[weather]\n",
    "\n",
    "weather.weather_main = weather.weather_main.apply(weather_to_binary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Weather data is hourly, so we use floor division to move from the UNIX timestamp (in seconds) to the integer hours. This is the case for both real time data and weather. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "trip_live['hourly'] = trip_live['time'] // 3600\n",
    "weather['hourly'] = weather['timestamp'] // 3600\n",
    "weather.drop('timestamp', axis = 1, inplace=True)\n",
    "trip_live = trip_live.merge(weather, how='inner', on='hourly')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:04:59.209074Z",
     "start_time": "2023-09-07T22:04:54.138932Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Put together sequential updates\n",
    "\n",
    "Now we put together information related to sequential updates, each (disjoint) pair of records/stops in a trip is put together.\n",
    "\n",
    "The trick is copying the dataset and then shifting the `stop_sequence` by -1. At this point, an inner join between the two dataset (the copied and the original one) is enough. Suffixes `_pre` and `_post` are enough to distinguish the two sets of fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "trip_live_shifted = trip_live.copy()\n",
    "trip_live_shifted['stop_sequence'] = trip_live_shifted['stop_sequence'] - 1\n",
    "trip_live = trip_live.merge(trip_live_shifted, how = 'inner', on = ['route_id', 'trip_id', 'start_time', 'start_date', 'stop_sequence'], suffixes=('_pre', '_post'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:24.821065Z",
     "start_time": "2023-09-07T22:04:59.462768Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add stop info and (actual) distances\n",
    "\n",
    "Now we add stop info, both for what concerns stop specific information and for what concerns the stop sequence. With `stop_times` we also get the [travelled distance in-between stops](https://developers.google.com/transit/gtfs/reference?hl=en#stop_timestxt). The problem is that the foreign key for `stop_times` is trip-related, and static data on trips is continuously updated (once a day); therefore, we cannot really do an inner join based on `trip_id` for what concerns dynamic/scraped data, in **any case**.\n",
    "\n",
    "The idea is the following:\n",
    "\n",
    "- You first perform an inner join between `stop_times` file and `trip_info`, based on `trip_id`s; we are more or less sure that this is consistent, since the text files come from the same GET request.\n",
    "- We again use the trick of subtracting 1 to the stop sequence in order to join two stops in a sequence in the same row. This is basically an inner join of the dataframe with itself, in a way.\n",
    "- We end up with a dataframe linking `route_id`s, `stop_id`s of the departing stops, `stop_id`s of the arrival stops and the distance travelled along the shape for each piece of route/edge. \n",
    "- At this point, finally, we can perform an inner join between the `trip_live` dataframe and the one we have been building here, using as key four attributes: [`route_id`, `stop_id_pre`, `stop_id_post`].\n",
    "\n",
    "After everything, we simply compute the actual distance between stops by taking the difference between the cumulative traveled distance fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "route_stop = trip_info.merge(stop_times, on='trip_id')\n",
    "route_stop_shifted = route_stop.copy()\n",
    "route_stop_shifted['stop_sequence'] -= 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:26.457572Z",
     "start_time": "2023-09-07T22:05:24.815345Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "route_stop = route_stop.merge(route_stop_shifted, on = ['trip_id', 'route_id', 'stop_sequence'], suffixes=('_pre', '_post'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:29.028832Z",
     "start_time": "2023-09-07T22:05:26.458404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "route_stop.rename(columns={'stop_sequence':'stop_sequence_pre'}, inplace=True)\n",
    "route_stop = route_stop[['route_id', 'stop_id_pre', 'stop_id_post', 'shape_dist_traveled_pre', 'shape_dist_traveled_post']]\n",
    "route_stop = route_stop.drop_duplicates(subset=['route_id', 'stop_id_pre', 'stop_id_post'], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:29.654826Z",
     "start_time": "2023-09-07T22:05:29.026478Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "trip_live = trip_live.merge(route_stop, on=['route_id', 'stop_id_pre', 'stop_id_post'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:42.031604Z",
     "start_time": "2023-09-07T22:05:29.649739Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "trip_live['stop_distance'] = trip_live['shape_dist_traveled_post']-trip_live['shape_dist_traveled_pre']\n",
    "del route_stop, route_stop_shifted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:42.274464Z",
     "start_time": "2023-09-07T22:05:42.010759Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add day of the week\n",
    "\n",
    "We add the day of the week (as integer) in order to be more specific when building the (filtered) graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "trip_live['time_pre_datetime'] = pd.to_datetime(trip_live['time_pre'], origin='unix', unit = 's', utc=True).dt.tz_convert('Europe/Rome')\n",
    "trip_live['day_of_week'] = trip_live.time_pre_datetime.dt.weekday"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:43.485575Z",
     "start_time": "2023-09-07T22:05:42.249681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add time\n",
    "We take the difference between (prediction) timestamps to get the elapsed time between stops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "trip_live['elapsed'] = trip_live['time_post']-trip_live['time_pre']\n",
    "trip_live = trip_live[trip_live['elapsed'] > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:48.647979Z",
     "start_time": "2023-09-07T22:05:43.482761Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before continuing, we remove from `trip_live` the columns which are not necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "trip_live = trip_live[['weather_main_post', 'day_of_week', 'time_pre_datetime', 'elapsed', 'stop_distance', 'stop_id_post']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:49.876246Z",
     "start_time": "2023-09-07T22:05:48.645592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "trip_live.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:49.889042Z",
     "start_time": "2023-09-07T22:05:49.882133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "trip_live.to_feather('trip_live.feather')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:50.734424Z",
     "start_time": "2023-09-07T22:05:49.887986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "         weather_main_post  day_of_week         time_pre_datetime  elapsed  \\\n0                   Clouds            5 2023-05-06 16:54:52+02:00       72   \n1                    Clear            5 2023-05-06 17:32:40+02:00       86   \n2                    Clear            5 2023-05-06 18:15:46+02:00       76   \n3                    Clear            5 2023-05-06 18:40:48+02:00       54   \n4                    Clear            5 2023-05-06 19:00:56+02:00       40   \n...                    ...          ...                       ...      ...   \n15041192             Clear            3 2023-06-15 03:00:20+02:00      388   \n15041193             Clear            1 2023-06-13 03:07:00+02:00       88   \n15041194             Clear            1 2023-06-13 03:36:22+02:00      580   \n15041195             Clear            6 2023-05-07 01:00:42+02:00      216   \n15041196             Clear            3 2023-05-25 01:03:30+02:00      128   \n\n          stop_distance stop_id_post  \n0                   340        20215  \n1                   340        20215  \n2                   340        20215  \n3                   340        20215  \n4                   340        20215  \n...                 ...          ...  \n15041192           5208        71212  \n15041193           5208        71212  \n15041194           5208        71212  \n15041195           1468        71460  \n15041196            590        70604  \n\n[15041197 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weather_main_post</th>\n      <th>day_of_week</th>\n      <th>time_pre_datetime</th>\n      <th>elapsed</th>\n      <th>stop_distance</th>\n      <th>stop_id_post</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Clouds</td>\n      <td>5</td>\n      <td>2023-05-06 16:54:52+02:00</td>\n      <td>72</td>\n      <td>340</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Clear</td>\n      <td>5</td>\n      <td>2023-05-06 17:32:40+02:00</td>\n      <td>86</td>\n      <td>340</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clear</td>\n      <td>5</td>\n      <td>2023-05-06 18:15:46+02:00</td>\n      <td>76</td>\n      <td>340</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Clear</td>\n      <td>5</td>\n      <td>2023-05-06 18:40:48+02:00</td>\n      <td>54</td>\n      <td>340</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clear</td>\n      <td>5</td>\n      <td>2023-05-06 19:00:56+02:00</td>\n      <td>40</td>\n      <td>340</td>\n      <td>20215</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15041192</th>\n      <td>Clear</td>\n      <td>3</td>\n      <td>2023-06-15 03:00:20+02:00</td>\n      <td>388</td>\n      <td>5208</td>\n      <td>71212</td>\n    </tr>\n    <tr>\n      <th>15041193</th>\n      <td>Clear</td>\n      <td>1</td>\n      <td>2023-06-13 03:07:00+02:00</td>\n      <td>88</td>\n      <td>5208</td>\n      <td>71212</td>\n    </tr>\n    <tr>\n      <th>15041194</th>\n      <td>Clear</td>\n      <td>1</td>\n      <td>2023-06-13 03:36:22+02:00</td>\n      <td>580</td>\n      <td>5208</td>\n      <td>71212</td>\n    </tr>\n    <tr>\n      <th>15041195</th>\n      <td>Clear</td>\n      <td>6</td>\n      <td>2023-05-07 01:00:42+02:00</td>\n      <td>216</td>\n      <td>1468</td>\n      <td>71460</td>\n    </tr>\n    <tr>\n      <th>15041196</th>\n      <td>Clear</td>\n      <td>3</td>\n      <td>2023-05-25 01:03:30+02:00</td>\n      <td>128</td>\n      <td>590</td>\n      <td>70604</td>\n    </tr>\n  </tbody>\n</table>\n<p>15041197 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_live"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:50.789945Z",
     "start_time": "2023-09-07T22:05:50.735465Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trend filtering, validation and application\n",
    "## Function to build the signal over the graph vertexes\n",
    "The following is a function to define the signal over the vertexes. The way we are defining the signal over the vertex set is the following: \n",
    "> The signal for each vertex is the average value (averaging along the observations gathered through some filtering options) of the elapsed time to get to a specific vertex from the previous divided by the (shape) distance between the previous vertex to the next.\n",
    "\n",
    "The resulting graph is thus related only to a specific filtering option (based on weather, day of the week and time of the day). We allow the choice of only one filtering option, in order to have enough observations to estimate meaningful average of the signal instances. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def vertex_signal(complete_df: pd.DataFrame, routes_graph: nx.Graph, *, weather: Optional[int] = None,\n",
    "                  day: Optional[int] = None, time: Optional[tuple[int, int]] = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Function assigning signal over the public transport graph vertexes by averaging across the inbound edges\n",
    "    elapsed time, according to a specific filtering option, passed as keyword argument.\n",
    "    :param complete_df: The dataframe containing the preprocessed data.\n",
    "    :param routes_graph: The graph, already built.\n",
    "    :param weather: Main weather conditions, either 0 or 1.\n",
    "    :param day: The day of the week, as integer in the range [0, 6].\n",
    "    :param time: The daytime, as an interval specified by a tuple of two integers.\n",
    "    :return: The graph with the signal defined over the vertex set.\n",
    "    \"\"\"\n",
    "    routes_graph = routes_graph.copy()\n",
    "\n",
    "    if sum([(weather is None), (day is None), (time is None)]) != 2:\n",
    "        raise TypeError(\n",
    "            'This functions builds the graph according to only one filtering option, you have to pass one and only one.')\n",
    "    if weather is not None:\n",
    "        mask = (complete_df['weather_main_post'] == weather)\n",
    "    elif day is not None:\n",
    "        mask = (complete_df['day_of_week'] == day)\n",
    "    else:\n",
    "        start_time, end_time = pd.to_datetime(time[0]).time(), pd.to_datetime(time[1]).time()\n",
    "        mask = ((complete_df.time_pre_datetime.dt.time >= start_time) & (complete_df.time_pre_datetime.dt.time <= end_time))\n",
    "\n",
    "    complete_df = complete_df[mask]\n",
    "    if not len(complete_df):\n",
    "        raise (ValueError('The filtering option you passed is wrong since no observation has matching fields.'))\n",
    "\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    complete_df['elapsed'] /= complete_df['stop_distance']\n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "    complete_df = complete_df[['elapsed', 'stop_id_post']].groupby('stop_id_post').mean()\n",
    "    nx.set_node_attributes(routes_graph, complete_df.to_dict('index'))\n",
    "\n",
    "    delete_vx = [x[0] for x in routes_graph.nodes('elapsed') if x[1] is None]\n",
    "    routes_graph.remove_nodes_from(delete_vx)\n",
    "    routes_graph.remove_nodes_from(list(nx.isolates(routes_graph)))\n",
    "    return routes_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:50.791189Z",
     "start_time": "2023-09-07T22:05:50.768861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "for node in nx.isolates(init_graph):\n",
    "    print(node)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:50.791458Z",
     "start_time": "2023-09-07T22:05:50.769369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['weather_main_post', 'day_of_week', 'time_pre_datetime', 'elapsed',\n       'stop_distance', 'stop_id_post'],\n      dtype='object')"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_live.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:50.792554Z",
     "start_time": "2023-09-07T22:05:50.769542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "test_graph = vertex_signal(trip_live, init_graph, wtr = 'clear')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:51.694933Z",
     "start_time": "2023-09-07T22:05:51.282259Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trend filtering test, piecewise linear case ($D = \\Delta^{(2)})$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having built the graph, we define the function producing the difference operator from the graph according to [Tibshirani R. et al. (2015)](https://jmlr.org/papers/volume17/15-147/15-147.pdf)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "import scipy\n",
    "def difference_op(graph: nx.Graph, order: int) -> scipy.sparse.csr_array:\n",
    "    \"\"\"\n",
    "    Produces a linear difference operator for graph trend filtering according to Tibshirani R. et al. (2015).\n",
    "    :param graph: The graph from which to build the difference linear operator.\n",
    "    :param order: The order of the difference operator.\n",
    "    :return: The difference operator as a SciPy sparse row matrix.\n",
    "    \"\"\"\n",
    "    if order == 1:\n",
    "        out = nx.incidence_matrix(graph, oriented=True)\n",
    "    elif order == 2:\n",
    "        out = nx.laplacian_matrix(graph)\n",
    "    elif not order % 2:\n",
    "        out = scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**(order/2)\n",
    "    else:\n",
    "        out = (nx.incidence_matrix(graph, oriented=True) @ \n",
    "               scipy.sparse.csr_matrix(nx.laplacian_matrix(graph))**((order-1)/2))\n",
    "    \n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:51.701202Z",
     "start_time": "2023-09-07T22:05:51.694452Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, [remember](https://jmlr.org/papers/volume17/15-147/15-147.pdf) that what we are aiming at is a non-parametric regression in a Generalized Lasso problem fashion:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\beta} & \\quad \\frac{1}{2} \\lVert Y - \\beta \\rVert_2^2 + \\lambda \\lVert D \\beta \\rVert_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Where $D$ is an arbitrary difference (linear) operator specifying the signal structure we are enforcing through L1 penalization. The first term of the loss function is strictly convex in $\\beta$, while the second term is convex in $\\beta$ being a composition of a convex mapping and a linear mapping in $\\beta$: therefore the problem is (strictly) convex and has a (unique) solution, being the loss coercive in $\\beta$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "difference = difference_op(test_graph, 2)\n",
    "vector_time = np.array([x[1] for x in test_graph.nodes(data = 'elapsed')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:51.712491Z",
     "start_time": "2023-09-07T22:05:51.708651Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to get the solution out, since this is a convex problem, we use CVXPY with the CVXOPT solver, and more info can be found [here](https://github.com/elsonidoq/py-l1tf/blob/master/l1tf/impl.py)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "vlambda = 0.1 # Choosing the regularization hyperparameter\n",
    "x = cp.Variable(shape=len(vector_time)) # Variable\n",
    "obj = cp.Minimize((1/2) * cp.sum_squares(vector_time - x)\n",
    "                  + vlambda * cp.norm(difference @ x, 1) ) # defining the optimization problem\n",
    "prob = cp.Problem(obj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T22:05:51.729877Z",
     "start_time": "2023-09-07T22:05:51.713949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.2                                    \n",
      "===============================================================================\n",
      "(CVXPY) Sep 07 12:56:29 PM: Your problem has 5688 variables, 0 constraints, and 0 parameters.\n",
      "(CVXPY) Sep 07 12:56:29 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Sep 07 12:56:29 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Sep 07 12:56:29 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 07 12:56:29 PM: Compiling problem (target solver=CVXOPT).\n",
      "(CVXPY) Sep 07 12:56:29 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> CVXOPT\n",
      "(CVXPY) Sep 07 12:56:29 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Sep 07 12:56:29 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Sep 07 12:56:29 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Sep 07 12:56:29 PM: Applying reduction CVXOPT\n",
      "(CVXPY) Sep 07 12:56:29 PM: Finished problem compilation (took 2.326e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 07 12:56:29 PM: Invoking solver CVXOPT  to obtain a solution.\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -1.0000e+00  2e+06  6e+01  2e+01  1e+00\n",
      " 1:  6.3199e+02  6.3200e+02  2e+06  7e+01  2e+01  2e+00\n",
      " 2:  4.2694e+01  1.4523e+02  2e+06  7e+01  2e+01  1e+02\n",
      " 3: -1.6228e+03  2.3547e+02  1e+05  2e+01  6e+00  2e+03\n",
      " 4: -1.5186e+03  3.7145e+02  2e+04  7e+00  2e+00  2e+03\n",
      " 5: -1.2419e+03  3.8338e+02  1e+04  5e+00  2e+00  2e+03\n",
      " 6: -6.1889e+02  4.0709e+02  5e+03  3e+00  9e-01  1e+03\n",
      " 7: -3.0789e+02  4.3197e+02  3e+03  2e+00  6e-01  7e+02\n",
      " 8:  1.8987e+01  4.5355e+02  2e+03  1e+00  4e-01  4e+02\n",
      " 9:  1.8735e+02  4.7551e+02  1e+03  8e-01  3e-01  3e+02\n",
      "10:  3.5269e+02  4.9406e+02  6e+02  4e-01  1e-01  1e+02\n",
      "11:  4.4836e+02  5.1045e+02  3e+02  2e-01  7e-02  6e+01\n",
      "12:  4.9439e+02  5.2128e+02  2e+02  1e-01  4e-02  3e+01\n",
      "13:  5.2767e+02  5.3202e+02  6e+01  3e-02  1e-02  4e+00\n",
      "14:  5.3588e+02  5.3674e+02  2e+01  1e-02  3e-03  9e-01\n",
      "15:  5.3841e+02  5.3853e+02  5e+00  3e-03  9e-04  1e-01\n",
      "16:  5.3904e+02  5.3906e+02  1e+00  6e-04  2e-04  2e-02\n",
      "17:  5.3917e+02  5.3917e+02  4e-01  2e-04  8e-05  5e-03\n",
      "18:  5.3921e+02  5.3922e+02  1e-01  7e-05  3e-05  1e-03\n",
      "19:  5.3923e+02  5.3923e+02  3e-02  2e-05  5e-06  2e-04\n",
      "20:  5.3924e+02  5.3924e+02  5e-03  3e-06  1e-06  4e-05\n",
      "21:  5.3924e+02  5.3924e+02  6e-04  3e-07  1e-07  2e-06\n",
      "22:  5.3924e+02  5.3924e+02  1e-04  6e-08  2e-08  3e-07\n",
      "Optimal solution found.\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 07 01:09:35 PM: Problem status: optimal\n",
      "(CVXPY) Sep 07 01:09:35 PM: Optimal value: 5.392e+02\n",
      "(CVXPY) Sep 07 01:09:35 PM: Compilation took 2.326e-02 seconds\n",
      "(CVXPY) Sep 07 01:09:35 PM: Solver (including time spent in interface) took 7.860e+02 seconds\n",
      "Solver status: optimal\n"
     ]
    }
   ],
   "source": [
    "prob.solve(solver = cp.CVXOPT, verbose = True)\n",
    "print('Solver status: {}'.format(prob.status))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:09:35.709075Z",
     "start_time": "2023-09-07T10:56:29.668272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.32698475, 0.36641572, 0.25845404, ..., 0.33481919, 0.34364668,\n       0.23764177])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:35.795019Z",
     "start_time": "2023-09-07T11:10:35.789353Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "congestion_dict = dict(zip(test_graph.nodes, x.value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:38.220852Z",
     "start_time": "2023-09-07T11:10:38.215356Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the trend-filtering mess\n",
    "\n",
    "Our idea to validate the model in order to get a good $\\lambda$ is the following:\n",
    "- First of all we split the data in a train and a validation set, according to the time axis. The holdout set consists of 8 days, being the 20% of the overall data, starting from June 9th onwards.\n",
    "- Once this is done, we perform trend filtering on the graph built with a single filter specification, with a specific choice of $\\lambda$. What we get back is a dictionary, mapping each stop to its congestion value, the signal we are modelling defined as above.\n",
    "- Now we take the holdout set, and we filter it according to the same filter we have used in order to build the graph to be trend filtered. \n",
    "- For each resulting row, we consider the arrival stop and the distance between the stops. We multiply the distance by the estimated congestion signal of the stop. Then we take the difference between the estimated elapsed time and the actual (forecasted...) elapsed time. Then we take the average of the absolute values of that quantity, which is our validation metric.\n",
    "- We end up with a different chosen $\\lambda$ for each filtering option. This makes sense, since the assumption that a unique $\\lambda$ would work for all filtering options cannot hold; think about intervals of time when it rains, they are of course sparser than sunny or cloudy time periods, and it is thus more than reasonable that the information estimated when it rains is generally less representative. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "val_mask = trip_live['time_pre_datetime'] >= pd.to_datetime('2023-06-09').tz_localize(\"Europe/Rome\")\n",
    "train_data = trip_live[~val_mask]\n",
    "val_data = trip_live[val_mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:44.945658Z",
     "start_time": "2023-09-07T11:10:44.344650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def trend_filter_validate(train: pd.DataFrame, val: pd.DataFrame, routes_graph: nx.Graph, lambda_seq: tuple[float, ...],\n",
    "                          cond_filter: tuple) -> Dict[float, np.ndarray]:\n",
    "    \"\"\"Runs a validation using trend filtering on a given train-test split.\n",
    "\n",
    "    :arg\n",
    "        train (pd.DataFrame): the training data.\n",
    "        val (pd.DataFrame): the validation data.\n",
    "        routes_graph (nx.Graph): the networkx graph of bus routes.\n",
    "        lambda_seq (tuple[float, ...]): the sequence of lambda values to try.\n",
    "        cond_filter (tuple[float, ...]): the filter used to select validation data. A tuple with a key that is either\n",
    "        \"weather\", \"day\", \"time\", and a corresponding value, e.g. (\"day\", 0) for Monday.\n",
    "\n",
    "    :return\n",
    "        (dict) a dictionary with validation metrics.\n",
    "    \"\"\"\n",
    "    cond_filter_dict = {cond_filter[0]: cond_filter[1]}\n",
    "\n",
    "    # Building the unfiltered graph on training data\n",
    "    train_graph = vertex_signal(train, routes_graph, **cond_filter_dict)\n",
    "\n",
    "    difference_operator = difference_op(train_graph, 2)\n",
    "    time_vec = np.array([x[1] for x in train_graph.nodes(data='elapsed')])\n",
    "    metric_dict = {}\n",
    "\n",
    "    # Filtering the validation data\n",
    "    if cond_filter[0] == 'weather':\n",
    "        mask = (val['weather_main_post'] == cond_filter[1])\n",
    "    elif cond_filter[0] == 'day':\n",
    "        mask = (val['day_of_week'] == cond_filter[1])\n",
    "    elif cond_filter[0] == 'time':\n",
    "        start_time, end_time = pd.to_datetime(cond_filter[0]).time(), pd.to_datetime(cond_filter[1]).time()\n",
    "        mask = ((val_data.time_pre_datetime.dt.time >= start_time) & (val_data.time_pre_datetime.dt.time <= end_time))\n",
    "    else:\n",
    "        raise ValueError('Illegal filtering option.')\n",
    "    val = val[mask]\n",
    "\n",
    "    for value_lambda in lambda_seq:\n",
    "        # Filtering on training data\n",
    "        x = cp.Variable(shape=len(time_vec))\n",
    "        loss = cp.Minimize((1 / 2) * cp.sum_squares(time_vec - x)\n",
    "                           + value_lambda * cp.norm(difference_operator @ x, 1))\n",
    "        problem = cp.Problem(loss)\n",
    "        problem.solve(solver=cp.CVXOPT, verbose=False)\n",
    "        congestion_df = pd.DataFrame(zip(train_graph.nodes, x.value), columns=['stop_id_post', 'congestion'])\n",
    "\n",
    "        # Compute validation metric for specific lambda\n",
    "        val_congestion = val.merge(congestion_df, on='stop_id_post')\n",
    "        error = np.absolute(val_congestion['congestion'] * val_congestion['stop_distance'] - val_congestion['elapsed'])\n",
    "        metric_dict[value_lambda] = error\n",
    "\n",
    "    return metric_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:47.684963Z",
     "start_time": "2023-09-07T11:10:47.677528Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The application\n",
    "\n",
    "Once we have chosen the right $\\lambda$ for each filter option, we can real-time query the system according to the required conditions (time, weather and day of the week), the system then retrieves the specific trend filtered sets of congestion values (three sets each time). In order to get unique values for each inbound stop/vertex, we perform a convex combination between the values coming from the three different sets (coming from three different filters). built for each of them. If a stop is missing from any of the sets (it may well be), we simply rescale the convex combination weights and consider only the sets where the stop is present. "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
